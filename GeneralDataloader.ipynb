{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneralDataloader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7l8K7wRodmq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload() # expire any previous token(s) and upload recreated token\n",
        "\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download andrewmvd/face-mask-detection\n",
        "!unzip -q face-mask-detection.zip -d face-mask-detection # outputs supressed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "iiIOw2kF1ZkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ceCkzp-S-6"
      },
      "outputs": [],
      "source": [
        "class GestureDataset:\n",
        "\n",
        "    CLASSES = {\"with_mask\": 0,\n",
        "               \"without_mask\": 1,\n",
        "               \"mask_weared_incorrect\": 2}\n",
        "\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "\n",
        "        self.image_dir = os.path.join(dataset_path, \"images\")\n",
        "        self.xml_dir = os.path.join(dataset_path, \"annotations\")\n",
        "        self.image_list = os.listdir(self.image_dir)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load an image and an annotation\n",
        "        \"\"\"\n",
        "        img_name = self.image_list[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = transforms.ToTensor()(img)\n",
        "\n",
        "        bbox, labels = self.read_xml(img_name, self.xml_dir)\n",
        "        boxes = torch.as_tensor(bbox, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = dict()\n",
        "\n",
        "        target['boxes'] = boxes\n",
        "        target['label'] = labels\n",
        "        target['image_id'] = torch.tensor([idx])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def read_xml(self, file_name, xml_dir):\n",
        "        \"\"\"\n",
        "        Function used to get the bounding boxes and labels from the xml file\n",
        "        Input:\n",
        "            file_name: image file name\n",
        "            xml_dir: directory of xml file\n",
        "        Return:\n",
        "            bbox : list of bounding boxes\n",
        "            labels: list of labels\n",
        "        \"\"\"\n",
        "        bboxes = []\n",
        "        labels = []\n",
        "\n",
        "        annot_path = os.path.join(xml_dir, file_name[:-3] + 'xml')\n",
        "        tree = ET.parse(annot_path)\n",
        "        root = tree.getroot()\n",
        "        for boxes in root.iter('object'):\n",
        "            ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
        "            xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
        "            ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
        "            xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
        "            label = boxes.find('name').text\n",
        "\n",
        "            label_idx = GestureDataset.CLASSES[label]\n",
        "            bboxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "            labels.append(label_idx)\n",
        "\n",
        "        return bboxes, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "def data_loader(batch_size):\n",
        "  gesture_dataset = GestureDataset('/content/face-mask-detection/')\n",
        "\n",
        "  # Divide dataset into test, train and validation subsets\n",
        "  image_num = len(gesture_dataset)\n",
        "  index_list = list(range(0, image_num))\n",
        "  test_split = int(len(index_list) * 0.9) # 10% of dataset for testing\n",
        "  val_split = int(test_split * 0.8) # 20% of remaining for validation\n",
        "\n",
        "  # Shuffle to make the allocation random\n",
        "  np.random.seed(2343)\n",
        "  np.random.shuffle(index_list)\n",
        "  train_indices, validation_indices, test_indices = index_list[:val_split], index_list[val_split:test_split], index_list[test_split:]\n",
        "  \n",
        "  train_sample = SubsetRandomSampler(train_indices)\n",
        "  train_set = torch.utils.data.DataLoader(gesture_dataset, batch_size=batch_size, num_workers=1, sampler=train_sample)\n",
        "\n",
        "  validation_sample = SubsetRandomSampler(validation_indices)\n",
        "  validation_set = torch.utils.data.DataLoader(gesture_dataset, batch_size=batch_size, num_workers=1, sampler=validation_sample)\n",
        "\n",
        "  test_sample = SubsetRandomSampler(test_indices)\n",
        "  test_set = torch.utils.data.DataLoader(gesture_dataset, batch_size=batch_size, num_workers=1, sampler=test_sample)\n",
        "\n",
        "  return train_set, validation_set, test_set\n"
      ],
      "metadata": {
        "id": "fiv1jtpx0ECs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}